{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cab5faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e5fbc8",
   "metadata": {},
   "source": [
    "## <span style=\"color: aquamarine;\"> Functions</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3298bf0e",
   "metadata": {},
   "source": [
    "### <span style=\"color: yellow\">Text Extraction</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "195892ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_pdfplumber(path):\n",
    "    text = \"\"\n",
    "\n",
    "    with pdfplumber.open(path) as file:\n",
    "        print(f\"Total pages: {len(file.pages)}\")\n",
    "\n",
    "        for page_num, page in enumerate(file.pages):\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text += page_text\n",
    "                text += f\"\\n--- Page {page_num +1} ---\\n\"\n",
    "\n",
    "    # filename = os.path.splitext(os.path.basename(path))[0]\n",
    "    # output_path = f\"output/{filename}.txt\"\n",
    "        \n",
    "    # with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    #     f.write(text)\n",
    "    \n",
    "    #     print(f\"✓ {filename}: {len(text)} characters\")\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612237f3",
   "metadata": {},
   "source": [
    "### <span style=\"color: yellow\">Text Cleaning</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "719c73ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, remove_page_markers=True):\n",
    "    if not text:\n",
    "        print(\"No text to clean\")\n",
    "        return \"\"\n",
    "    \n",
    "    original_length = len(text)\n",
    "    print(f\"Original length: {len(text):,} characters\")\n",
    "\n",
    "\n",
    "    # Remove page markers if requested\n",
    "    if remove_page_markers:\n",
    "        text = re.sub(r'\\n--- Page \\d+ ---\\n', '\\n', text)\n",
    "    \n",
    "    #Remove common headers/ footers\n",
    "    text = re.sub(r'Page \\d+ of \\d+', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'^\\d+\\s*$', '', text, flags=re.MULTILINE) \n",
    "\n",
    "    #Remove excessive whitespace\n",
    "    text = re.sub(r' +', ' ', text)  # Multiple spaces to single space\n",
    "    text = re.sub(r'\\t+', ' ', text)  # Tabs to spaces\n",
    "\n",
    "   #Fix newlines (keep paragraph breaks, remove excessive ones)\n",
    "    text = re.sub(r'\\n\\s*\\n\\s*\\n+', '\\n\\n', text)  # Max 2 newlines\n",
    "    \n",
    "    # Remove special characters that might cause issues (optional)\n",
    "    # Uncomment if you see weird symbols\n",
    "    # text = re.sub(r'[^\\w\\s.,!?;:()\\-\\'\\\"$%@#&/\\\\]', '', text)\n",
    "    \n",
    "    #Fix common PDF extraction issues\n",
    "    text = text.replace('\\x00', '')  # Remove null characters\n",
    "    text = text.replace('\\uf0b7', '•')  # Fix bullet points\n",
    "    text = text.replace('\\u2019', \"'\")  # Fix apostrophes\n",
    "    text = text.replace('\\u201c', '\"')  # Fix quotes\n",
    "    text = text.replace('\\u201d', '\"')  # Fix quotes\n",
    "    \n",
    "    #Strip leading/trailing whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    cleaned_length = len(text)\n",
    "    print(f\"Cleaned length: {len(text):,} characters\")\n",
    "    print(f\"Removed: {original_length - cleaned_length:,} characters\")\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a0d229",
   "metadata": {},
   "source": [
    "### <span style=\"color: yellow\">Text Chunking</span>\n",
    "\n",
    "#### <span style=\"color: pink\">This step is to split text into chunks of approximately chunk_size words. Parameters include text (text to chunk), chunk_size (target words per chunk), and overlap (words to overlap between chunks).</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f17fdf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=600, overlap=100):\n",
    "    words = text.split()\n",
    "\n",
    "    chunks = []\n",
    "    start = 0\n",
    "\n",
    "    while start < len(words):\n",
    "        end = start+chunk_size\n",
    "        chunk_words = words[start:end]\n",
    "\n",
    "        chunk=' '.join(chunk_words)\n",
    "        chunks.append(chunk)\n",
    "\n",
    "        start+=chunk_size-overlap\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23602242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 63 chunks from sample text\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"This is a sample text. \" * 1000  \n",
    "sample_chunks = chunk_text(sample_text, chunk_size=100, overlap=20)\n",
    "print(f\"Created {len(sample_chunks)} chunks from sample text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e16ce0",
   "metadata": {},
   "source": [
    "### <span style=\"color: yellow\">Storing Documents</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fcc71c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_document_store(cleaned_texts, company_name=\"delta\", chunk_size=600, overlap=100):\n",
    "    \n",
    "    all_chunks = []\n",
    "    chunk_counter = 1\n",
    "    \n",
    "    print(\"\\nCreating document store...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for source_file, text in cleaned_texts.items():\n",
    "        print(f\"\\nProcessing: {source_file}\")\n",
    "        \n",
    "        # Chunk the text\n",
    "        chunks = chunk_text(text, chunk_size=chunk_size, overlap=overlap)\n",
    "        print(f\"  Created {len(chunks)} chunks\")\n",
    "        \n",
    "        # Add each chunk to the list\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunk_data = {\n",
    "                'chunk_id': f\"chunk_{chunk_counter:03d}\",\n",
    "                'company': company_name,\n",
    "                'source_file': source_file,\n",
    "                'chunk_text': chunk\n",
    "            }\n",
    "            all_chunks.append(chunk_data)\n",
    "            chunk_counter += 1\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(all_chunks)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"✓ Document store created!\")\n",
    "    print(f\"  Total chunks: {len(df)}\")\n",
    "    print(f\"  Total documents: {len(cleaned_texts)}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def43abf",
   "metadata": {},
   "source": [
    "## <span style=\"color: aquamarine;\"> Processing Files</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1256310",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_files = {\n",
    "    \"annual_report_2020\": \"input/10K Report 2020.pdf\",\n",
    "    \"annual_report_2021\": \"input/10K Report 2021.pdf\",\n",
    "    \"annual_report_2022\": \"input/10K Report 2022.pdf\",\n",
    "    \"annual_report_2023\": \"input/10K Report 2023.pdf\",\n",
    "    \"annual_report_2024\": \"input/10K Report 2024.pdf\",\n",
    "    \"esg_report_2020\": \"input/ESG Report 2020.pdf\",\n",
    "    \"esg_report_2021\": \"input/ESG Report 2021.pdf\",\n",
    "    \"esg_report_2022\": \"input/ESG Report 2022.pdf\",\n",
    "    \"esg_report_2023\": \"input/ESG Report 2023.pdf\",\n",
    "    \"esg_report_2024\": \"input/ESG Report 2024.pdf\",\n",
    "    \"major_holders\": \"input/Major Holders Summary.pdf\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c45cf050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 1: EXTRACTING AND CLEANING PDFs\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Processing: annual_report_2020\n",
      "============================================================\n",
      "Total pages: 125\n",
      "\n",
      "Cleaning...\n",
      "Original length: 456,922 characters\n",
      "Cleaned length: 454,904 characters\n",
      "Removed: 2,018 characters\n",
      "✓ Saved as annual_report_2020_clean.txt\n",
      "\n",
      "============================================================\n",
      "Processing: annual_report_2021\n",
      "============================================================\n",
      "Total pages: 123\n",
      "\n",
      "Cleaning...\n",
      "Original length: 438,207 characters\n",
      "Cleaned length: 436,221 characters\n",
      "Removed: 1,986 characters\n",
      "✓ Saved as annual_report_2021_clean.txt\n",
      "\n",
      "============================================================\n",
      "Processing: annual_report_2022\n",
      "============================================================\n",
      "Total pages: 115\n",
      "\n",
      "Cleaning...\n",
      "Original length: 410,076 characters\n",
      "Cleaned length: 408,226 characters\n",
      "Removed: 1,850 characters\n",
      "✓ Saved as annual_report_2022_clean.txt\n",
      "\n",
      "============================================================\n",
      "Processing: annual_report_2023\n",
      "============================================================\n",
      "Total pages: 109\n",
      "\n",
      "Cleaning...\n",
      "Original length: 381,346 characters\n",
      "Cleaned length: 379,599 characters\n",
      "Removed: 1,747 characters\n",
      "✓ Saved as annual_report_2023_clean.txt\n",
      "\n",
      "============================================================\n",
      "Processing: annual_report_2024\n",
      "============================================================\n",
      "Total pages: 104\n",
      "\n",
      "Cleaning...\n",
      "Original length: 363,871 characters\n",
      "Cleaned length: 362,206 characters\n",
      "Removed: 1,665 characters\n",
      "✓ Saved as annual_report_2024_clean.txt\n",
      "\n",
      "============================================================\n",
      "Processing: esg_report_2020\n",
      "============================================================\n",
      "Total pages: 63\n",
      "\n",
      "Cleaning...\n",
      "Original length: 149,614 characters\n",
      "Cleaned length: 148,245 characters\n",
      "Removed: 1,369 characters\n",
      "✓ Saved as esg_report_2020_clean.txt\n",
      "\n",
      "============================================================\n",
      "Processing: esg_report_2021\n",
      "============================================================\n",
      "Total pages: 73\n",
      "\n",
      "Cleaning...\n",
      "Original length: 164,537 characters\n",
      "Cleaned length: 163,357 characters\n",
      "Removed: 1,180 characters\n",
      "✓ Saved as esg_report_2021_clean.txt\n",
      "\n",
      "============================================================\n",
      "Processing: esg_report_2022\n",
      "============================================================\n",
      "Total pages: 91\n",
      "\n",
      "Cleaning...\n",
      "Original length: 233,994 characters\n",
      "Cleaned length: 232,505 characters\n",
      "Removed: 1,489 characters\n",
      "✓ Saved as esg_report_2022_clean.txt\n",
      "\n",
      "============================================================\n",
      "Processing: esg_report_2023\n",
      "============================================================\n",
      "Total pages: 66\n",
      "\n",
      "Cleaning...\n",
      "Original length: 188,729 characters\n",
      "Cleaned length: 187,628 characters\n",
      "Removed: 1,101 characters\n",
      "✓ Saved as esg_report_2023_clean.txt\n",
      "\n",
      "============================================================\n",
      "Processing: esg_report_2024\n",
      "============================================================\n",
      "Total pages: 59\n",
      "\n",
      "Cleaning...\n",
      "Original length: 155,925 characters\n",
      "Cleaned length: 154,968 characters\n",
      "Removed: 957 characters\n",
      "✓ Saved as esg_report_2024_clean.txt\n",
      "\n",
      "============================================================\n",
      "Processing: major_holders\n",
      "============================================================\n",
      "Total pages: 2\n",
      "\n",
      "Cleaning...\n",
      "Original length: 3,058 characters\n",
      "Cleaned length: 3,027 characters\n",
      "Removed: 31 characters\n",
      "✓ Saved as major_holders_clean.txt\n",
      "\n",
      "✓ All files extracted and cleaned!\n",
      "\n",
      "============================================================\n",
      "STEP 2: CREATING DOCUMENT STORE (CHUNKING)\n",
      "============================================================\n",
      "\n",
      "Creating document store...\n",
      "============================================================\n",
      "\n",
      "Processing: annual_report_2020\n",
      "  Created 140 chunks\n",
      "\n",
      "Processing: annual_report_2021\n",
      "  Created 134 chunks\n",
      "\n",
      "Processing: annual_report_2022\n",
      "  Created 125 chunks\n",
      "\n",
      "Processing: annual_report_2023\n",
      "  Created 116 chunks\n",
      "\n",
      "Processing: annual_report_2024\n",
      "  Created 111 chunks\n",
      "\n",
      "Processing: esg_report_2020\n",
      "  Created 46 chunks\n",
      "\n",
      "Processing: esg_report_2021\n",
      "  Created 49 chunks\n",
      "\n",
      "Processing: esg_report_2022\n",
      "  Created 67 chunks\n",
      "\n",
      "Processing: esg_report_2023\n",
      "  Created 55 chunks\n",
      "\n",
      "Processing: esg_report_2024\n",
      "  Created 47 chunks\n",
      "\n",
      "Processing: major_holders\n",
      "  Created 1 chunks\n",
      "\n",
      "============================================================\n",
      "✓ Document store created!\n",
      "  Total chunks: 891\n",
      "  Total documents: 11\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "DOCUMENT STORE STATISTICS\n",
      "============================================================\n",
      "\n",
      "Total chunks: 891\n",
      "\n",
      "Chunks per source file:\n",
      "source_file\n",
      "annual_report_2020    140\n",
      "annual_report_2021    134\n",
      "annual_report_2022    125\n",
      "annual_report_2023    116\n",
      "annual_report_2024    111\n",
      "esg_report_2020        46\n",
      "esg_report_2021        49\n",
      "esg_report_2022        67\n",
      "esg_report_2023        55\n",
      "esg_report_2024        47\n",
      "major_holders           1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Average chunk length: 596 words\n",
      "Min chunk length: 5 words\n",
      "Max chunk length: 600 words\n",
      "\n",
      "============================================================\n",
      "SAMPLE CHUNKS\n",
      "============================================================\n",
      "    chunk_id company         source_file  \\\n",
      "0  chunk_001   delta  annual_report_2020   \n",
      "1  chunk_002   delta  annual_report_2020   \n",
      "2  chunk_003   delta  annual_report_2020   \n",
      "\n",
      "                                          chunk_text  \n",
      "0  UNITED STATES SECURITIES AND EXCHANGE COMMISSI...  \n",
      "1  þ The aggregate market value of the voting and...  \n",
      "2  the date of this report. Delta Air Lines, Inc....  \n",
      "\n",
      "============================================================\n",
      "SAMPLE CHUNK TEXT\n",
      "============================================================\n",
      "\n",
      "First 500 characters of chunk_001:\n",
      "UNITED STATES SECURITIES AND EXCHANGE COMMISSION Washington, D.C. 20549 FORM 10-K ☑ ANNUAL REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT OF 1934 For the fiscal year ended December 31, 2020 or ☐ TRANSITION REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT OF 1934 Commission File Number 001-5424 DELTA AIR LINES, INC. (Exact name of registrant as specified in its charter) Delaware 58-0218548 (State or other jurisdiction of incorporation or organization) (\n",
      "\n",
      "============================================================\n",
      "STEP 3: SAVING DOCUMENT STORE\n",
      "============================================================\n",
      "✓ Saved to CSV: data/delta/chunks.csv\n",
      "✓ Saved to Pickle: data/delta/chunks.pkl\n",
      "\n",
      "============================================================\n",
      "✓ PIPELINE COMPLETE!\n",
      "============================================================\n",
      "\n",
      "You now have:\n",
      "  - 11 cleaned text files in 'output/'\n",
      "  - 891 chunks in 'data/delta/chunks.csv'\n",
      "  - Document store ready for retrieval\n"
     ]
    }
   ],
   "source": [
    "cleaned_texts = {}\n",
    "os.makedirs(\"output\", exist_ok=True)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STEP 1: EXTRACTING AND CLEANING PDFs\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for file_name, file_path in input_files.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing: {file_name}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    # Extract\n",
    "    raw_text = extract_text_pdfplumber(file_path)\n",
    "    \n",
    "    # Clean\n",
    "    print(\"\\nCleaning...\")\n",
    "    cleaned_text = clean_text(raw_text)\n",
    "    cleaned_texts[file_name] = cleaned_text\n",
    "    \n",
    "    # Save cleaned text\n",
    "    clean_output_path = f\"output/{file_name}_clean.txt\"\n",
    "    with open(clean_output_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(cleaned_text)\n",
    "    print(f\"✓ Saved as {file_name}_clean.txt\")\n",
    "\n",
    "print(\"\\n✓ All files extracted and cleaned!\")\n",
    "\n",
    "# Step 3: Create document store\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 2: CREATING DOCUMENT STORE (CHUNKING)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "document_store = create_document_store(\n",
    "    cleaned_texts, \n",
    "    company_name=\"delta\",\n",
    "    chunk_size=600,  # Adjust between 500-1000\n",
    "    overlap=100       # Adjust overlap as needed\n",
    ")\n",
    "\n",
    "# Step 4: Display statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DOCUMENT STORE STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nTotal chunks: {len(document_store)}\")\n",
    "print(f\"\\nChunks per source file:\")\n",
    "print(document_store['source_file'].value_counts().sort_index())\n",
    "\n",
    "print(f\"\\nAverage chunk length: {document_store['chunk_text'].str.split().str.len().mean():.0f} words\")\n",
    "print(f\"Min chunk length: {document_store['chunk_text'].str.split().str.len().min()} words\")\n",
    "print(f\"Max chunk length: {document_store['chunk_text'].str.split().str.len().max()} words\")\n",
    "\n",
    "# Step 5: Display sample chunks\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAMPLE CHUNKS\")\n",
    "print(\"=\"*60)\n",
    "print(document_store.head(3))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAMPLE CHUNK TEXT\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nFirst 500 characters of chunk_001:\")\n",
    "print(document_store.loc[0, 'chunk_text'][:500])\n",
    "\n",
    "# Step 6: Save document store\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 3: SAVING DOCUMENT STORE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "os.makedirs(\"data/delta\", exist_ok=True)\n",
    "\n",
    "# Save as CSV\n",
    "csv_path = \"data/delta/chunks.csv\"\n",
    "document_store.to_csv(csv_path, index=False)\n",
    "print(f\"✓ Saved to CSV: {csv_path}\")\n",
    "\n",
    "# Save as pickle (better for Python)\n",
    "pkl_path = \"data/delta/chunks.pkl\"\n",
    "document_store.to_pickle(pkl_path)\n",
    "print(f\"✓ Saved to Pickle: {pkl_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✓ PIPELINE COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nYou now have:\")\n",
    "print(f\"  - {len(cleaned_texts)} cleaned text files in 'output/'\")\n",
    "print(f\"  - {len(document_store)} chunks in 'data/delta/chunks.csv'\")\n",
    "print(f\"  - Document store ready for retrieval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b116fd",
   "metadata": {},
   "source": [
    "### <span style=\"color: yellow;\"> Adjusting Chunk Sizes</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36e7bddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating document store...\n",
      "============================================================\n",
      "\n",
      "Processing: annual_report_2020\n",
      "  Created 155 chunks\n",
      "\n",
      "Processing: annual_report_2021\n",
      "  Created 149 chunks\n",
      "\n",
      "Processing: annual_report_2022\n",
      "  Created 139 chunks\n",
      "\n",
      "Processing: annual_report_2023\n",
      "  Created 129 chunks\n",
      "\n",
      "Processing: annual_report_2024\n",
      "  Created 123 chunks\n",
      "\n",
      "Processing: esg_report_2020\n",
      "  Created 51 chunks\n",
      "\n",
      "Processing: esg_report_2021\n",
      "  Created 54 chunks\n",
      "\n",
      "Processing: esg_report_2022\n",
      "  Created 74 chunks\n",
      "\n",
      "Processing: esg_report_2023\n",
      "  Created 62 chunks\n",
      "\n",
      "Processing: esg_report_2024\n",
      "  Created 52 chunks\n",
      "\n",
      "Processing: major_holders\n",
      "  Created 2 chunks\n",
      "\n",
      "============================================================\n",
      "✓ Document store created!\n",
      "  Total chunks: 990\n",
      "  Total documents: 11\n",
      "============================================================\n",
      "\n",
      "Creating document store...\n",
      "============================================================\n",
      "\n",
      "Processing: annual_report_2020\n",
      "  Created 82 chunks\n",
      "\n",
      "Processing: annual_report_2021\n",
      "  Created 79 chunks\n",
      "\n",
      "Processing: annual_report_2022\n",
      "  Created 74 chunks\n",
      "\n",
      "Processing: annual_report_2023\n",
      "  Created 69 chunks\n",
      "\n",
      "Processing: annual_report_2024\n",
      "  Created 65 chunks\n",
      "\n",
      "Processing: esg_report_2020\n",
      "  Created 27 chunks\n",
      "\n",
      "Processing: esg_report_2021\n",
      "  Created 29 chunks\n",
      "\n",
      "Processing: esg_report_2022\n",
      "  Created 40 chunks\n",
      "\n",
      "Processing: esg_report_2023\n",
      "  Created 33 chunks\n",
      "\n",
      "Processing: esg_report_2024\n",
      "  Created 28 chunks\n",
      "\n",
      "Processing: major_holders\n",
      "  Created 1 chunks\n",
      "\n",
      "============================================================\n",
      "✓ Document store created!\n",
      "  Total chunks: 527\n",
      "  Total documents: 11\n",
      "============================================================\n",
      "Small chunks (500 words): 990 chunks\n",
      "Medium chunks (600 words): 891 chunks\n",
      "Large chunks (1000 words): 527 chunks\n"
     ]
    }
   ],
   "source": [
    "# Smaller chunks (500 words)\n",
    "document_store_small = create_document_store(cleaned_texts, chunk_size=500, overlap=50)\n",
    "\n",
    "# Larger chunks (1000 words)\n",
    "document_store_large = create_document_store(cleaned_texts, chunk_size=1000, overlap=150)\n",
    "\n",
    "# Compare\n",
    "print(f\"Small chunks (500 words): {len(document_store_small)} chunks\")\n",
    "print(f\"Medium chunks (600 words): {len(document_store)} chunks\")\n",
    "print(f\"Large chunks (1000 words): {len(document_store_large)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e827b2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the document store\n",
    "# df = pd.read_csv(\"data/delta/chunks.csv\")\n",
    "\n",
    "# # Check structure\n",
    "# print(\"DataFrame Info:\")\n",
    "# print(df.info())\n",
    "\n",
    "# print(\"\\nFirst few rows:\")\n",
    "# print(df.head())\n",
    "\n",
    "# print(\"\\nSample chunk:\")\n",
    "# print(f\"\\nChunk ID: {df.loc[0, 'chunk_id']}\")\n",
    "# print(f\"Company: {df.loc[0, 'company']}\")\n",
    "# print(f\"Source: {df.loc[0, 'source_file']}\")\n",
    "# print(f\"Text preview: {df.loc[0, 'chunk_text'][:300]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c6fe9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pages: 125\n",
      "Total pages: 123\n",
      "Total pages: 115\n",
      "Total pages: 109\n",
      "Total pages: 104\n",
      "Total pages: 63\n",
      "Total pages: 73\n",
      "Total pages: 91\n",
      "Total pages: 66\n",
      "Total pages: 59\n",
      "Total pages: 2\n"
     ]
    }
   ],
   "source": [
    "#Convert to text files\n",
    "# raw_annual_report_2020 = extract_text_pdfplumber(\"input/10K Report 2020.pdf\")\n",
    "# raw_annual_report_2021 = extract_text_pdfplumber(\"input/10K Report 2021.pdf\")\n",
    "# raw_annual_report_2022 = extract_text_pdfplumber(\"input/10K Report 2022.pdf\")\n",
    "# raw_annual_report_2023 = extract_text_pdfplumber(\"input/10K Report 2023.pdf\")\n",
    "# raw_annual_report_2024 = extract_text_pdfplumber(\"input/10K Report 2024.pdf\")\n",
    "\n",
    "# raw_esg_report_2020 = extract_text_pdfplumber(\"input/ESG Report 2020.pdf\")\n",
    "# raw_esg_report_2021 = extract_text_pdfplumber(\"input/ESG Report 2021.pdf\")\n",
    "# raw_esg_report_2022 = extract_text_pdfplumber(\"input/ESG Report 2022.pdf\")\n",
    "# raw_esg_report_2023 = extract_text_pdfplumber(\"input/ESG Report 2023.pdf\")\n",
    "# raw_esg_report_2024 = extract_text_pdfplumber(\"input/ESG Report 2024.pdf\")\n",
    "\n",
    "# raw_major_holders = extract_text_pdfplumber(\"input/Major Holders Summary.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7278c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# filename = os.path.splitext(os.path.basename(annual_report_2020))[0]\n",
    "# clean_output_path = f\"output/{annual_report_2020}_clean.txt\"\n",
    "\n",
    "# with open(clean_output_path, 'w', encoding='utf-8') as f:\n",
    "#     f.write(clean_text(annual_report_2020))\n",
    "\n",
    "# print(f\"\\nCleaned text saved to: {clean_output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
